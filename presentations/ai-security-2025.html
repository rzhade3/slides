<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>AI Security in 2025</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
                <section id="title">
                    <h1>AI Security in 2025</h1>
                    <h3>Or: How I learned to stop worrying and love the agent</h3>
                </section>
                <section id="about-me">
                    <h2>About Me</h2>
                    <ul>
                        <li><strong>Rahul Zhade</strong></li>
                        <li>Product Security Engineer</li>
                        <li><a href="https://github.com/rzhade3">GitHub</a></li>
                    </ul>
                </section>
                <section id="agenda">
                    <h2>Agenda</h2>
                    <ol>
                        <li>Intro to AI</li>
                        <li>SDLC Vulnerabilities from AI</li>
                        <li>Product Vulnerabilities through AI</li>
                        <li>MCP Security</li>
                        <li>Responsible AI Design</li>
                        <li>Incident Response</li>
                    </ol>
                </section>
                <section id="intro-to-ai">
                    <section>
                        <h2>Module: Intro to AI</h2>
                    </section>
                    <section>
                        <h3>AI Lifecycle</h3>
                        <img src="../public/ai-security-2025/ai-lifecycle.png" alt="AI Lifecycle">
                    </section>
                    <section>
                        <h3>Secure AI Framework</h3>
                        <img src="../public/ai-security-2025/saif.png" alt="Secure AI Framework screenshot">
                    </section>
                    <section>
                        <h3>AI vs Traditional Security</h3>
                        <ul>
                            <li>Unstructured Inputs</li>
                            <li>Side Channels</li>
                            <li>Non-determinism</li>
                            <li>Lack of explainability</li>
                            <li>Greenfield! Lack of standards</li>
                        </ul>
                    </section>
                    <section>
                        <h3>AI Usages in Software</h3>
                        <ul>
                            <li><strong>Constructive</strong>: Copilot, Cursor, etc.</li>
                            <li><strong>Defensive</strong>: Copilot Suggestions</li>
                            <li><strong>Integrated</strong>: Rest of this talk :)</li>
                        </ul>
                    </section>
                </section>
                <section id="security-considerations">
                    <section>
                        <h2>Module: Security Considerations for AI SDLC</h2>
                    </section>
                    <section>
                        <h3>AI Usages in SDLC</h3>
                        <ul>
                            <li><strong>Constructive</strong>: Copilot, Cursor, etc.</li>
                            <li><strong>Defensive</strong>: CI, Copilot Suggestions, etc.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Constructive AI Risks</h3>
                        <ul>
                            <li>Prone to hallucinations (e.g., Package squatting)</li>
                            <li>Can scoop up data (Context may contain sensitive info)</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Controls to Apply</h3>
                        <ul>
                            <li>Properly review suggested code</li>
                            <li>Use context exclusions</li>
                            <li>Plan architecture in advance</li>
                            <li>Add context via comments</li>
                            <li>Use static/dynamic analysis</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Defensive AI</h3>
                        <ul>
                            <li>More context-aware than SAST</li>
                            <li>Easier rule writing</li>
                            <li>Still prone to hallucinations, prompt injection</li>
                        </ul>
                    </section>
                </section>
                <section id="product-vulnerabilities">
                    <section>
                        <h2>Module: Product Vulnerabilities</h2>
                    </section>
                    <section>
                        <h3>Types of Vulnerabilities</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Data Lifecycle</th>
                                    <th>Infrastructure</th>
                                    <th>Emergent Issues</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Sensitive Info Disclosure</td>
                                    <td>Supply Chain Vulns</td>
                                    <td>Prompt Injection</td>
                                </tr>
                                <tr>
                                    <td>Data/Model Poisoning</td>
                                    <td>Unbounded Consumption</td>
                                    <td>Improper Output Handling</td>
                                </tr>
                                <tr>
                                    <td></td>
                                    <td>Model Theft</td>
                                    <td>Excessive Agency</td>
                                </tr>
                                <tr>
                                    <td></td>
                                    <td></td>
                                    <td>Hallucination</td>
                                </tr>
                            </tbody>
                        </table>
                    </section>
                    <section>
                        <h3>Data Lifecycle</h3>
                        <blockquote>AI is leaky!</blockquote>
                        <ul>
                            <li>Inputs can be repeated verbatim</li>
                            <li>Side channels</li>
                            <li>Huge training datasets</li>
                            <li><strong>Control:</strong> Only use trusted, secure data</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Data Lifecycle Controls</h3>
                        <ul>
                            <li>PII Redaction</li>
                            <li>Careful Data Selection</li>
                            <li>Differential Privacy (if possible)</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Infrastructure</h3>
                        <blockquote>AI Infrastructure is non-trivial</blockquote>
                        <ul>
                            <li>Expensive</li>
                            <li>Bespoke dependencies</li>
                            <li>Unsafe defaults</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Infrastructure Controls</h3>
                        <ul>
                            <li>CI/CD pipelines (e.g., Wiz)</li>
                            <li>Rate limiting</li>
                            <li>Logging</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Emergent Issues</h3>
                        <blockquote>People don't know how to use AI</blockquote>
                        <ul>
                            <li>Ambiguous intent</li>
                            <li>Misunderstood APIs</li>
                            <li>Prompt Injection, Excessive Agency, etc.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Improper Output Handling</h3>
                        <blockquote>Output from LLMs must be sanitized</blockquote>
                        <img src="../public/ai-security-2025/xss.png" alt="Improper Output Handling">
                    </section>
                    <section>
                        <h3>Controls</h3>
                        <ul>
                            <li>Context-aware output encoding</li>
                            <li>Don't allow unsanitized output in sensitive contexts (UIs, shells)</li>
                            <li>Use HITL for state changes</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Excessive Agency</h3>
                        <blockquote>AI may hallucinate or take wrong actions</blockquote>
                        <img src="../public/ai-security-2025/excessive-agency.png" alt="Excessive Agency">
                    </section>
                    <section>
                        <h3>Controls</h3>
                        <ul>
                            <li>Human in the Loop (HITL)</li>
                            <li>Responsible agent design</li>
                            <li>Educate users</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Prompt Injection</h3>
                        <img src="../public/ai-security-2025/prompt-injection.png" alt="Prompt Injection">
                    </section>
                    <section>
                        <h3>Types of Prompt Injection</h3>
                        <ul>
                            <li><strong>User Prompt Injection (UPIA)</strong>: Jailbreaks, etc.</li>
                            <li><strong>Cross Prompt Injection (XPIA)</strong>: MCP Vulnerabilities, etc.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Prompt Injection Controls</h3>
                        <ul>
                            <li>Clearer context</li>
                            <li>Content filtering</li>
                            <li>Dual LLM paradigm</li>
                            <li>Nanny Agents</li>
                            <li><a href="https://arxiv.org/abs/2503.18813">CaMeL</a>/ Information Control Flow</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Hallucination / Misinformation</h3>
                        <ul>
                            <li>Inform users of AI limitations</li>
                            <li>HITL: Keep human in the loop</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Hallucination Controls</h3>
                        <ul>
                            <li>Use Retrieval Augmented Generation (RAG)</li>
                            <li>Off topic filtering</li>
                        </ul>
                    </section>
                </section>
                <section id="mcp-security">
                    <section>
                        <h2>Module: MCP Security</h2>
                    </section>
                    <section>
                        <h3>Issues with MCP</h3>
                        <ul>
                            <li>Authorization</li>
                            <li>Supply Chain</li>
                            <li>XPIA Risk</li>
                            <li>Context Poisoning</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Using MCP Safely</h3>
                        <ol>
                            <li>Audit source code for any local MCP</li>
                            <li>Use credentials with least privilege</li>
                            <li>Use firewalls to ensure agents stay on guardrails</li>
                            <li>Properly define resources and tools</li>
                        </ol>
                    </section>
                    <section>
                        <h3>MCP Security Specifications</h3>
                        <ul>
                            <li><a href="https://arxiv.org/html/2506.01333v1">Enhanced Tool Definition Interface</a></li>
                            <li>Oauth 2.0 Support</li>
                        </ul>
                    </section>
                </section>
                <section id="responsible-ai">
                    <section>
                        <h2>Module: Responsible AI</h2>
                    </section>
                    <section>
                        <h3>Good Design Prevents Footguns</h3>
                    </section>
                    <section>
                        <h3>Human in the Loop</h3>
                        <ul>
                            <li>Protect users from themselves</li>
                        </ul>
                    </section>
                    <section>
                        <h3>AI Disclosures</h3>
                        <ul>
                            <li>Disclose all AI usage</li>
                            <li>Explicit user consent for state-changing actions</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Content Filtering</h3>
                        <ul>
                            <li>Prevent harmful/off-topic content</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Agentic Security Principles</h3>
                        <ol>
                            <li>Make sure agents can be turned off</li>
                            <li>Network interactions should be logged</li>
                            <li>Log who initiated the action, and input context</li>
                            <li>Run workflows in ephemeral environments</li>
                            <li>Do not use user secrets unless explicitly defined</li>
                        </ol>
                    </section>
                </section>
                <section id="incident-response">
                    <section>
                        <h2>Module: Incident Response for AI</h2>
                    </section>
                    <section>
                        <h3>Principles</h3>
                        <ul>
                            <li>Logging & Monitoring</li>
                            <li>Rate Limits</li>
                            <li>Generous Logging (Reproducibility is hard)</li>
                            <li>Set Customer Expectations</li>
                        </ul>
                    </section>
                </section>
                <section id="conclusion">
                    <h2>Conclusion</h2>
                    <section>
                        <h3>Key Takeaways</h3>
                        <ul>
                            <li>Secure the whole AI lifecycle</li>
                            <li><strong>Responsible AI Design!</strong>
                                <ul>
                                    <li>HITL HITL HITL</li>
                                </ul>
                            </li>
                            <li>Same risks, higher stakes</li>
                            <li>Don't trust AI blindly</li>
                        </ul>
                    </section>
                </section>
                <section id="appendix">
                    <h2>Appendix</h2>
                    <ul>
                        <li><a href="https://github.com/rzhade3/adversarial-ai-reading-list">Adversarial AI Reading List</a></li>
                        <li><a href="https://embracethered.com">embracethered.com</a></li>
                        <li><a href="https://simonwillison.net/tags/security/">Simon Willison Blog</a></li>
                    </ul>
                </section>
			</div>
		</div>

		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script>
			Reveal.initialize({
				hash: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
