<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>AI Security in 2025</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
                <section id="title">
                    <h1>AI Security in 2025</h1>
                    <h3>Or: How I learned to stop worrying and love the agent</h3>
                </section>
                <section id="about-me">
                    <h2>About Me</h2>
                    <ul>
                        <li>Product Security Engineer</li>
                        <li><strong>Rahul Zhade</strong></li>
                        <li><a href="https://github.com/rzhade3">GitHub</a></li>
                    </ul>
                </section>
                <section id="agenda">
                    <h2>Agenda</h2>
                    <ol>
                        <li>Intro to AI</li>
                        <li>SDLC Vulnerabilities from AI</li>
                        <li>Product Vulnerabilities through AI</li>
                        <li>Responsible AI Design</li>
                        <li>Incident Response</li>
                    </ol>
                </section>
                <section id="intro-to-ai">
                    <h2>Module: Intro to AI</h2>
                    <section>
                        <h3>AI Lifecycle</h3>
                        <img src="../public/ai-security-2025/ai-lifecycle.png" alt="AI Lifecycle">
                    </section>
                    <section>
                        <h3>Secure AI Framework</h3>
                        <img src="../public/ai-security-2025/saif.png" alt="Secure AI Framework screenshot">
                    </section>
                    <section>
                        <h3>AI vs Traditional Security</h3>
                        <ul>
                            <li>Unstructured Inputs</li>
                            <li>Side Channels</li>
                            <li>Non-determinism</li>
                            <li>Lack of explainability</li>
                            <li>Greenfield! Lack of standards</li>
                        </ul>
                    </section>
                    <section>
                        <h3>AI Usages in Software</h3>
                        <ul>
                            <li><strong>Constructive</strong>: Copilot, Cursor, etc.</li>
                            <li><strong>Defensive</strong>: Copilot Suggestions</li>
                            <li><strong>Integrated</strong>: Rest of this talk :)</li>
                        </ul>
                    </section>
                </section>
                <section id="security-considerations">
                    <h2>Module: Security Considerations for AI SDLC</h2>
                    <section>
                        <h3>AI Usages in SDLC</h3>
                        <ul>
                            <li><strong>Constructive</strong>: Copilot, Cursor, etc.</li>
                            <li><strong>Defensive</strong>: CI, Copilot Suggestions, DryRun, etc.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Constructive AI Risks</h3>
                        <ul>
                            <li>Prone to hallucinations (e.g., Package squatting)</li>
                            <li>Can scoop up data (Context may contain sensitive info)</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Controls to Apply</h3>
                        <ul>
                            <li>Properly review suggested code</li>
                            <li>Use context exclusions</li>
                            <li>Plan architecture in advance</li>
                            <li>Add context via comments</li>
                            <li>Use static/dynamic analysis</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Defensive AI</h3>
                        <ul>
                            <li>More context-aware than SAST</li>
                            <li>Easier rule writing</li>
                            <li>Still prone to hallucinations, prompt injection</li>
                        </ul>
                    </section>
                </section>
                <section id="product-vulnerabilities">
                    <h2>Module: Product Vulnerabilities from AI</h2>
                    <section>
                        <h3>Types of Vulnerabilities</h3>
                        <ul>
                            <li><strong>Data Lifecycle</strong>: Sensitive Info Disclosure, Data/Model Poisoning</li>
                            <li><strong>Infrastructure</strong>: Supply Chain Vulns, Unbounded Consumption, Model Theft</li>
                            <li><strong>Emergent Issues</strong>: Prompt Injection, Improper Output Handling, Excessive Agency, Hallucination</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Data Lifecycle</h3>
                        <blockquote>AI is leaky!</blockquote>
                        <ul>
                            <li>Inputs can be repeated verbatim</li>
                            <li>Side channels</li>
                            <li>Huge training datasets</li>
                            <li><strong>Control:</strong> Only use trusted, secure data</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Data Lifecycle Controls</h3>
                        <ul>
                            <li>PII Redaction</li>
                            <li>Careful Data Selection</li>
                            <li>Differential Privacy (if needed)</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Infrastructure</h3>
                        <blockquote>AI Infrastructure is non-trivial</blockquote>
                        <ul>
                            <li>Expensive, custom</li>
                            <li>Unsafe defaults</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Infrastructure Controls</h3>
                        <ul>
                            <li>CI/CD pipelines (e.g., Wiz)</li>
                            <li>Rate limiting</li>
                            <li>Logging</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Emergent Issues</h3>
                        <blockquote>People don’t know how to use AI</blockquote>
                        <ul>
                            <li>Ambiguous intent</li>
                            <li>Misunderstood APIs</li>
                            <li>Prompt Injection, Excessive Agency, etc.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Improper Output Handling</h3>
                        <blockquote>Output from LLMs must be sanitized</blockquote>
                        <img src="../public/ai-security-2025/xss.png" alt="Improper Output Handling">
                    </section>
                    <section>
                        <h3>Controls</h3>
                        <ul>
                            <li>Context-aware output encoding</li>
                            <li>Don’t allow unsanitized output in sensitive contexts (UIs, shells)</li>
                            <li>Use HITL for state changes</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Excessive Agency</h3>
                        <blockquote>AI may hallucinate or take wrong actions</blockquote>
                        <img src="../public/ai-security-2025/excessive-agency.png" alt="Excessive Agency">
                    </section>
                    <section>
                        <h3>Controls</h3>
                        <ul>
                            <li>Human in the Loop (HITL)</li>
                            <li>Responsible agent design</li>
                            <li>Educate users</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Prompt Injection</h3>
                        <img src="../public/ai-security-2025/prompt-injection.png" alt="Prompt Injection">
                    </section>
                    <section>
                        <h3>Prompt Injection Controls</h3>
                        <ul>
                            <li>Clearer context</li>
                            <li>Content filtering</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Hallucination / Misinformation</h3>
                        <ul>
                            <li>Inform users of AI limitations</li>
                            <li>HITL: Keep human in the loop</li>
                        </ul>
                    </section>
                </section>
                <section id="responsible-ai">
                    <h2>Module: Responsible AI</h2>
                    <section>
                        <h3>Good Design Prevents Footguns</h3>
                    </section>
                    <section>
                        <h3>Human in the Loop</h3>
                        <ul>
                            <li>Protect users from themselves</li>
                        </ul>
                    </section>
                    <section>
                        <h3>AI Disclosures</h3>
                        <ul>
                            <li>Disclose all AI usage</li>
                            <li>Explicit user consent for state-changing actions</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Content Filtering</h3>
                        <ul>
                            <li>Prevent harmful/off-topic content</li>
                        </ul>
                    </section>
                </section>
                <section id="incident-response">
                    <h2>Module: Incident Response for AI</h2>
                    <section>
                        <h3>Principles</h3>
                        <ul>
                            <li>Logging & Monitoring</li>
                            <li>Rate Limits</li>
                            <li>Generous Logging (Reproducibility is hard)</li>
                            <li>Set Customer Expectations</li>
                        </ul>
                    </section>
                </section>
                <section id="conclusion">
                    <h2>Conclusion</h2>
                    <section>
                        <h3>Key Takeaways</h3>
                        <ul>
                            <li>Secure the whole AI lifecycle</li>
                            <li><strong>Responsible AI Design!</strong></li>
                            <li>Same risks, higher stakes</li>
                        </ul>
                    </section>
                </section>
                <section id="appendix">
                    <h2>Appendix</h2>
                    <ul>
                        <li><a href="https://github.com/rzhade3/adversarial-ai-reading-list">Adversarial AI Reading List</a></li>
                        <li><a href="https://saif.google">saif.google</a></li>
                        <li><a href="https://embracethered.com">embracethered.com</a></li>
                    </ul>
                </section>
			</div>
		</div>

		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script>
			Reveal.initialize({
				hash: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
